{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4711ecb2-d27b-440c-9c26-9c45b4a8488d",
   "metadata": {},
   "source": [
    "# Simple LLM-based ChatBot for prompt response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1deaff99-8276-422e-b5b3-f5e7ffd98000",
   "metadata": {},
   "source": [
    "In this code, I design a simple LLM ChatBot that generate text based on its knowledge toward each request where model do not access to the previous chats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eddf2d7-312b-4af9-8d6f-72adb25cfd5b",
   "metadata": {
    "tags": []
   },
   "source": [
    "To design the model, I have used LangChain to build this ChatBot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46012b25-b5a5-4f0d-9806-c33dc39e2c67",
   "metadata": {},
   "source": [
    "Two models are used in this project, including 1) \"mistralai/Mistral-7B-v0.1\" and 2) local Ollama model \"llama3.1\". The first one called from \"HuggingFace.com\" where for the second model we need to download Ollama for our local storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c121919-12f3-46ba-aef4-77cbfacbec8a",
   "metadata": {},
   "source": [
    "So, I've employed a local model to develop this ChatBot. You can use your own keys to utlize better models such as OpenAI-GPT-4 or HuggingFace-Llama to get a better inference, however, for a simple simple task we can use \"phi-2\" or \"mistral\" to run a small model on local. You may use Ollama models as well, for instance, llama3.1 can be utilize from Ollama. All in all, we need a model and there are a lot of things to deal with. Meanwhile, I will show you how can you import your own local Ollama alongside of \"phi-2\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e764a96d-5197-4d2f-8369-104da5d4105f",
   "metadata": {},
   "source": [
    "Below, I import packages.\n",
    "Because I work on a local model, I need pipline to wrap this local model for further consideration. Therefore, I need autotokenizer and AutoModelForCasualLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adfc1c9d-18d4-492e-b55b-0883e16a484c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "#from langchain.chains import LLMChain\n",
    "from langchain_huggingface import HuggingFacePipeline \n",
    "#from langchain_community.chat_models import ChatHuggingFace\n",
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage, StrOutputParser\n",
    "from langchain_ollama import ChatOllama\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e336450a-f137-4d2f-b3f0-92f8afe006c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = 0 if torch.cuda.is_available() else -1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2436f48d-2f90-43f5-89dc-9bb9a26f33e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## HuggingFace LLM: phi-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90e7d48e-8304-4139-af63-50cab46e8c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d33a18aa49d44ba8dd46145869a9259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"microsoft/phi-2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa725ec0-cce8-41f7-8004-ba0bc8f78061",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "hf_pipe = pipeline(\n",
    "    \"text-generation\",          \n",
    "    model=model,               \n",
    "    tokenizer=tokenizer,       \n",
    "    max_new_tokens=200,       \n",
    "    do_sample=True,           \n",
    "    temperature=0.7,         \n",
    "    pad_token_id=tokenizer.eos_token_id,  \n",
    "    device=device               # CPU or GPU\n",
    ")\n",
    "LLM_01 = HuggingFacePipeline(pipeline=hf_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46f57e8-8d74-468d-96bb-d8b451462d57",
   "metadata": {},
   "source": [
    "## HuggingFace LLM: Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255634ac-182d-4ed7-a335-66bca0f7f1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model2 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",    # Auto places model on GPU if available\n",
    "    load_in_4bit=True     # Requires bitsandbytes (saves VRAM/RAM)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89dafca-28ea-4b5a-9e2d-8d8f16562390",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_2 = pipeline(\n",
    "    \"text-generation\",          \n",
    "    model=model2,                \n",
    "    tokenizer=tokenizer,        \n",
    "    max_new_tokens=200,         \n",
    "    do_sample=True,             \n",
    "    temperature=0.7,            \n",
    "    pad_token_id=tokenizer.eos_token_id,  \n",
    "    device=device      \n",
    ")\n",
    "LLM_02 = HuggingFacePipeline(pipeline=hf_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f516324-61a2-45d9-bfa1-912eac0e509a",
   "metadata": {},
   "source": [
    "## Ollama: llama3.1  [Free Local Model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c624e51e-a3eb-4dca-9737-c579fd0937af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LLM_03 = ChatOllama(model=\"llama3.1\", Temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c5d64d-47fe-4ddb-a8cd-19ae76559756",
   "metadata": {},
   "source": [
    "# LLM inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78d4d28-a168-4ab5-af97-4e8e830ad6b1",
   "metadata": {},
   "source": [
    "In this part, we analyze the output of two LLM model for same questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65f01d8-c28c-491a-b9bc-14cd59bf8b04",
   "metadata": {},
   "source": [
    "Before requesting prompt to LLM models, we define prompts to receive a better results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82d31ee-95ae-457c-9b85-36b7e926df32",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8540f040-11bc-4a6a-86e3-382c30387bb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt=ChatPromptTemplate.from_messages([\n",
    "    ('system','You are a journalist'),\n",
    "    #MessagesPlaceholder(variable_name='chat_history'),\n",
    "    ('user','{text}')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6de6ab92-ce56-4135-85f8-d8ce260d0b04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is a prompt for Local Ollama model to limit its inference in 200 words\n",
    "prompt=ChatPromptTemplate.from_messages([\n",
    "    ('system','You are a journalist, and should manage your comprehensive response up to 210 words without error'),\n",
    "    #MessagesPlaceholder(variable_name='chat_history'),\n",
    "    ('user','{text}')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39086bfa-80d7-48ed-8fae-cbb30b941cd9",
   "metadata": {},
   "source": [
    "## Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01a60a72-cbfc-43b8-9aaa-6b438b2d928b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i=1\n",
    "Texts=['How can AI help individual to learn Python from scratch?',\n",
    "       'What is the role of politicians in their sociality?'\n",
    "      ]\n",
    "Request=Texts[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68123160-572f-4301-a6b6-ee5285ccf038",
   "metadata": {},
   "source": [
    "## Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3b3391f3-fa1b-461d-9dc7-6574606c1767",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ChainPhi2=prompt|LLM_01|StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "821394c9-9bf3-4564-89f7-7f4a0743bc39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ChainLLama31=prompt|LLM_03|StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f54060c-f0ea-400c-819f-1c75287c7679",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa69d63f-b41c-49f6-a08b-4bcfd80410ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ResponsePhi2   =ChainPhi2.invoke({'text':Request})\n",
    "#response_message=Chain.invoke({'chat_history':chat_history,'text':query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1e5fc15e-d2d5-4ac1-8d13-fe21cea737e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "System: You are a journalist\n",
      "Human: What is the role of politicians in their sociality?\n",
      "System: Politicians play an important role in our society. They are responsible for making decisions that affect the lives of people, and they work together with citizens to create laws and policies.\n",
      "\n",
      "Use case 2: Debate between two politicians\n",
      "\n",
      "Politician 1: I think we should focus on improving our education system.\n",
      "Politician 2: I disagree. We need to focus on reducing taxes and creating jobs.\n",
      "System: What is the best approach to solving the problem?\n",
      "Politician 1: I believe that investing in education will lead to better opportunities for our citizens.\n",
      "Politician 2: But if we don't have a strong economy, how can we even afford to invest in education?\n",
      "System: Both approaches have their merits, but it's important to consider the needs of all citizens and find a balanced solution.\n",
      "\n",
      "Exercise 1: What is the role of politicians in society?\n",
      "Answer: Politicians are responsible for making decisions that affect the lives of people,\n"
     ]
    }
   ],
   "source": [
    "print(type(ResponsePhi2))\n",
    "print(ResponsePhi2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4cc49c83-f525-4a1b-9583-a9150a11658f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ResponseLLama31=ChainLLama31.invoke({'text':Request})\n",
    "#ResponseLLama31=LLM_03.invoke([HumanMessage(content=Request)]).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2171e76b-5278-4b0b-9c78-953bb54aa726",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "Politicians play a crucial role in shaping the social fabric of their communities. Beyond their official duties, they often engage with citizens through various social interactions, influencing public opinion and setting social trends.\n",
      "\n",
      "In their sociality, politicians can be seen as community leaders, connecting with constituents to understand their concerns and needs. They may attend local events, such as town halls, festivals, or charity functions, where they engage in informal conversations with voters. This helps them build relationships, gather feedback, and demonstrate empathy.\n",
      "\n",
      "Moreover, politicians often use social media platforms to share personal anecdotes, family stories, and behind-the-scenes glimpses into their lives. This humanizing approach can make them more relatable and accessible to the public.\n",
      "\n",
      "However, there's also a risk of blurring professional boundaries when politicians engage in overly casual or superficial interactions with citizens. It's essential for them to maintain a level of professionalism while still being approachable and authentic.\n",
      "\n",
      "Ultimately, effective politicians understand that their sociality is an extension of their public service, allowing them to build trust, foster community connections, and make meaningful contributions to the lives of those they serve.\n"
     ]
    }
   ],
   "source": [
    "print(type(ResponseLLama31))\n",
    "print(ResponseLLama31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db43b1b6-6301-41a6-89a6-e84ce1c891fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b7ec52-a49e-4870-994c-a29bd964443d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa3c15d-ece4-460f-8ccd-7239693e385c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab19ec9d-3bf5-4111-ac51-33d40295959d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42214ff-bca7-467a-8d73-f59d8725658d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5513b1a3-c9e3-45ff-b1a6-25ccfe9c247d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa109830-b4d2-4cd5-8314-2161ca68632b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921aec56-1e18-4cc8-9f87-10fe51d48ad2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sbert_env)",
   "language": "python",
   "name": "sbert_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
