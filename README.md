# Simple LLM chatbot with ephemeral memory
It is a simple LLM project based on both online HuggingFace models, like 'mistral' and 'microsoft/phi2', and local Ollama models, such as 'llama3.1'. 
Therefore, everyone can use its local storage to work on an LLM model or use the freely available models on the HuggingFace platform.

## Preload
In this code, I design a simple LLM ChatBot that generates text based on its knowledge for each request, where the model does not have access to the previous chats.

To design the model, I have used LangChain to build this ChatBot.

Three models are suggested in this project, including 1) '''mistralai/Mistral-7B-v0.1''' & '''microsoft/phi2''' and 2) the local Ollama model '''llama3.1'''. I used "phi" for as huggingface as a simple selection for students who do not have a high volume on their RAM. The second model is Ollama's featured model: "Llama3.1".

As you may know, the knowledge representation from "llama3.1" is more accurate than "phi". This is because llama includes 7B parameters while phi is about 2B. I will show this accuracy to inform you of the decision that will be generated by the 7B and 2B pretrained models. 

## More models
I've employed a local model to develop this ChatBot. If you want to utilize other models such as "OpenAI-GPT-models", "DeepSeek", and so forth, you can simply use you own API-keys in sake of getting better inference; however, to show how you can run an LLM model, I have used "phi-2" and "llama3.1". All in all, we need a model, and there are a lot of things to deal with. 


# LLM models
Meanwhile, I will show you how you can import your own local Ollama alongside "phi-2".

Below, I import packages. Because I work on a local model, I need pipeline to wrap this local model for further consideration. Therefore, I need an autotokenizer and an AutoModelForCasualLM.
