# Simple LLM chatbot with ephemeral memory
It is a simple LLM project based on both online HuggingFace models, like 'mistral' and 'microsoft/phi2', and local Ollama models, such as 'llama3.1'. 
Therefore, everyone can use its local storage to work on an LLM model or use the freely available models on the HuggingFace platform.

## Preload
In this code, I design a simple LLM ChatBot that generates text based on its knowledge for each request, where the model does not have access to the previous chats.

To design the model, I have used LangChain to build this ChatBot.

Three models are suggested in this project, including 1) ```mistralai/Mistral-7B-v0.1``` & ```microsoft/phi2``` and 2) the local Ollama model ```llama3.1```. I used "phi" for as huggingface as a simple selection for students who do not have a high volume on their RAM. The second model is Ollama's featured model: "Llama3.1".

As you may know, the knowledge representation from "llama3.1" is more accurate than "phi". This is because llama includes 7B parameters while phi is about 2B. I will show this accuracy to inform you of the decision that will be generated by the 7B and 2B pretrained models. 

## More models
I've employed a local model to develop this ChatBot. If you want to utilize other models such as "OpenAI-GPT-models", "DeepSeek", and so forth, you can simply use your own API keys in the sake of getting better inference; however, to show how you can run an LLM model, I have used ```phi-2``` and ```llama3.1```. All in all, we need a model, and there are a lot of things to deal with. 


# LLM models
Initially, we need to import essential packages as follows:
```python
from langchain.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder 
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain_huggingface import HuggingFacePipeline 
from langchain.schema import SystemMessage, HumanMessage, AIMessage, StrOutputParser
from langchain_ollama import ChatOllama
```
where we use ```prompt``` to address our request in an explicit and accurate way. ```Transformers``` allows us to wrap the model. ```schema``` lets us distinguish the requests and responses. ```ChatOllama``` is the main package to use for chat with our local model, which is stored on our laptop, for instance.

If your laptop doesn't have a strong GPU or generally has no GPU on your system, you can use CPU to run the code.
To check it, you can use the following code:
```python
import torch
device = 0 if torch.cuda.is_available() else -1
```

## Microsoft/Phi
To import the ```phi``` model
```python
model_id = "microsoft/phi-2"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id)
```
If you use this code for the first time, your system tries to download the model from ```HuggingFace```.
In the next step, we wrap the model with ```pipeline``` as follows:
```python
hf_pipe = pipeline(
    "text-generation",          
    model=model,               
    tokenizer=tokenizer,       
    max_new_tokens=200,       
    do_sample=True,           
    temperature=0.7,         
    pad_token_id=tokenizer.eos_token_id,  
    device=device               # CPU or GPU
)
LLM_01 = HuggingFacePipeline(pipeline=hf_pipe)
```
```text-generation``` is the main role for this LLM. Accordingly, we can define different rules such as "summarizing", "sentiment", or "chat" to indicate the role of the mode. It worths to note that, each model may have its crucial demand. For example, we are unable to use ```phi2``` for chat or an important role. This is because it is a small model that couldn't do some tasks or introduce significant information.
```max_new_tokens```imposes a limitation on the number of tokens to be generated. ```do_sample``` makes the model generate different responses at the each time the request is made, if it is set to ```True```.

## Misral and other HuggingFace models
To use other models from HuggingFace, we can simply address their links by their specific names like ```mistralai/Mistral-7B-v0.1```![Link: https://huggingface.co/mistralai/Mistral-7B-v0.1] for 7B Mistral model, accordingly, all other thing is the same as the previous section for ```phi2```.
It is valuable to note that to use most of the models in ```HuggingFace.com```, we need to send a request to its host for usage. For example, using ```meta-llama/Llama-3.1-8B```[Link: https://huggingface.co/meta-llama/Llama-3.1-8B] require a permission from ```Facebook```.

## Local Ollama
To call our local ollama model, we can simply use
```python
LLM_03 = ChatOllama(model="llama3.1", Temperature=0.7)
```
where ```Temperature``` sets the accuracy of the response.

So, we define our models, and in the next sections, we develop our Chatbots.
# LLL Inference
Before we get a response from the models, we have to standardize our request in a way that clearly express our request. Thus, we need to define a ```prompt``` so as to request our appeal in a better way.
## Prompt
Consider the following prompot that I have prepared for my local 'llama3.1':
```python
prompt=ChatPromptTemplate.from_messages([
    ('system','You are a journalist, and should manage your comprehensive response up to 210 words without error'),
    ('user','{text}')
])
```
I use ```system``` to indicate the role of the model, which is a "journalist" here, and ```user``` to address the request from a client.
Although there would be a lot of request in a chat, to show how it works, I have highlighted a single request as a query like of ```'What is the role of politicians in their sociality?'``` to ask the model in advance.

## Chain
A chain is a scenario to compact the request with its prompt to the model. For example in
```ChainLLama31=prompt|LLM_03|StrOutputParser()```
I have built a chain by ```prompt```and ```LLM_03(llama3.1)```. ```StrOutputParser```causes the response to be rapidly available as a string. The response may include several structures such as 'content', 'metadata', and so forth. So, using "StrOutputParser" can help us manage the response as soon as possible by delivering a string.





















