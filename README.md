# Simple LLM chatbot with ephemeral memory
It is a simple LLM project based on both online HuggingFace models, like 'mistral' and 'microsoft/phi2', and local Ollama models, such as 'llama3.1'. 
Therefore, everyone can use its local storage to work on an LLM model or use the freely available models on the HuggingFace platform.

## Preload
In this code, I design a simple LLM ChatBot that generates text based on its knowledge for each request, where the model does not have access to the previous chats.

To design the model, I have used LangChain to build this ChatBot.

Three models are suggested in this project, including 1) ```mistralai/Mistral-7B-v0.1``` & ```microsoft/phi2``` and 2) the local Ollama model ```llama3.1```. I used "phi" for as huggingface as a simple selection for students who do not have a high volume on their RAM. The second model is Ollama's featured model: "Llama3.1".

As you may know, the knowledge representation from "llama3.1" is more accurate than "phi". This is because llama includes 7B parameters while phi is about 2B. I will show this accuracy to inform you of the decision that will be generated by the 7B and 2B pretrained models. 

## More models
I've employed a local model to develop this ChatBot. If you want to utilize other models such as "OpenAI-GPT-models", "DeepSeek", and so forth, you can simply use your own API keys in the sake of getting better inference; however, to show how you can run an LLM model, I have used ```phi-2``` and ```llama3.1```. All in all, we need a model, and there are a lot of things to deal with. 


# LLM models
Initially, we need to import essential packages as follows:
```python
from langchain.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder 
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain_huggingface import HuggingFacePipeline 
from langchain.schema import SystemMessage, HumanMessage, AIMessage, StrOutputParser
from langchain_ollama import ChatOllama
```
where we use ```prompt``` to address our request in an explicit and accurate way. ```Transformers``` allows us to wrap the model. ```schema``` lets us distinguish the requests and responses. ```ChatOllama``` is the main package to use for chat with our local model, which is stored on our laptop, for instance.

If your laptop doesn't have a strong GPU or generally has no GPU on your system, you can use CPU to run the code.
To check it, you can use the following code:
```python
import torch
device = 0 if torch.cuda.is_available() else -1
```

## Microsoft/Phi
To import the ```phi``` model
```python
model_id = "microsoft/phi-2"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id)
```
If you use this code for the first time, your system tries to download the model from ```HuggingFace```.
In the next step, we wrap the model with ```pipeline``` as follows:
```python
hf_pipe = pipeline(
    "text-generation",          
    model=model,               
    tokenizer=tokenizer,       
    max_new_tokens=200,       
    do_sample=True,           
    temperature=0.7,         
    pad_token_id=tokenizer.eos_token_id,  
    device=device               # CPU or GPU
)
LLM_01 = HuggingFacePipeline(pipeline=hf_pipe)
```
```text-generation``` is the main role for this LLM. Accordingly, we can define different rules such as "summarizing", "sentiment", or "chat" to indicate the role of the mode. It worths to note that, each model may have its crucial demand. For example, we are unable to use ```phi2``` for chat or an important role. This is because it is a small model that couldn't do some tasks or introduce significant information.
```max_new_tokens```imposes a limitation on the number of tokens to be generated. As you may know, ```token``` has a different meaning rather than ```word```. I mean, the number of ```tokens``` is not equal to the number of ```words```. If you didn't know this, please search their difference in Google. ```do_sample``` makes the model generate different responses at each time the request is made, if it is set to ```True```.

## Misral and other HuggingFace models
To use other models from HuggingFace, we can simply address their links by their specific names like ```mistralai/Mistral-7B-v0.1```[Link: https://huggingface.co/mistralai/Mistral-7B-v0.1] for 7B Mistral model, accordingly, all other thing is the same as the previous section for ```phi2```.
It is valuable to note that to use most of the models in ```HuggingFace.com```, we need to send a request to its host for usage. For example, using ```meta-llama/Llama-3.1-8B```[Link: https://huggingface.co/meta-llama/Llama-3.1-8B] require a permission from ```Facebook```.

## Local Ollama
To call our local ollama model, we can simply use
```python
LLM_03 = ChatOllama(model="llama3.1", Temperature=0.7)
```
where ```Temperature``` sets the accuracy of the response.

So, we define our models, and in the next sections, we develop our Chatbots.
# LLL Inference
Before we get a response from the models, we have to standardize our request in a way that clearly express our request. Thus, we need to define a ```prompt``` so as to request our appeal in a better way.
## Prompt
Consider the following prompot that I have prepared for my local 'llama3.1':
```python
prompt=ChatPromptTemplate.from_messages([
    ('system','You are a journalist, and should manage your comprehensive response up to 210 words without error'),
    ('user','{text}')
])
```
I use ```system``` to indicate the role of the model, which is a "journalist" here, and ```user``` to address the request from a client.
Although there would be a lot of request in a chat, to show how it works, I have highlighted a single request as a query like of ```'What is the role of politicians in their sociality?'``` to ask the model in advance.

## Chain
A chain is a scenario to compact the request with its prompt to the model. For example in
```ChainLLama31=prompt|LLM_03|StrOutputParser()```
I have built a chain by ```prompt```and ```LLM_03(llama3.1)```. ```StrOutputParser```causes the response to be rapidly available as a string. The response may include several structures such as 'content', 'metadata', and so forth. So, using "StrOutputParser" can help us manage the response as soon as possible by delivering a string.


## Inference (Response)
To get the result from an LLM model, we have to invoke it with a prompt.
Below, I ask ```phi2``` and ```llama3.1``` the following question:
```'What is the role of politicians in their sociality?'``` 
where we can find out their accuracy by analyzing their response.
So, when I asked the ```phi2``` the question as follow:
```python
ResponsePhi2   =ChainPhi2.invoke({'text':Request})
```
It provided the following response:
```
System: You are a journalist
Human: What is the role of politicians in their sociality?
System: Politicians play an important role in our society. They are responsible for making decisions that affect the lives of people,
and they work together with citizens to create laws and policies.

Use case 2: Debate between two politicians

Politician 1: I think we should focus on improving our education system.
Politician 2: I disagree. We need to focus on reducing taxes and creating jobs.
System: What is the best approach to solving the problem?
Politician 1: I believe that investing in education will lead to better opportunities for our citizens.
Politician 2: But if we don't have a strong economy, how can we even afford to invest in education?
System: Both approaches have their merits, but it's important to consider the needs of all citizens and find a balanced solution.

Exercise 1: What is the role of politicians in society?
Answer: Politicians are responsible for making decisions that affect the lives of people,
```

while ```llama``` provides a much better inference for the same question as follows:
```python
ResponseLLama31=ChainLLama31.invoke({'text':Request})
```

```
Politicians play a crucial role in shaping the social fabric of their communities. Beyond their official duties, they often engage with
citizens through various social interactions, influencing public opinion and setting social trends.

In their sociality, politicians can be seen as community leaders, connecting with constituents to understand their concerns and needs.
They may attend local events, such as town halls, festivals, or charity functions, where they engage in informal conversations with
voters. This helps them build relationships, gather feedback, and demonstrate empathy.

Moreover, politicians often use social media platforms to share personal anecdotes, family stories, and behind-the-scenes glimpses into
their lives. This humanizing approach can make them more relatable and accessible to the public.

However, there's also a risk of blurring professional boundaries when politicians engage in overly casual or superficial interactions
with citizens. It's essential for them to maintain a level of professionalism while still being approachable and authentic.

Ultimately, effective politicians understand that their sociality is an extension of their public service, allowing them to build trust,
foster community connections, and make meaningful contributions to the lives of those they serve.
```















